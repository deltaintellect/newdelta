---
output:
  pdf_document:
    includes:
      before_body: titlepage.sty    
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
    toc: yes
tables: true
header-includes:
    - \usepackage{setspace}\onehalfspacing
    - \usepackage{caption}


bibliography: reference.bib
---

\clearpage

```{r setup, include=FALSE}
options(knitr.duplicate.label = "allow")
knitr::opts_chunk$set(echo = TRUE)
library(psych)
library(ggplot2)
library(dplyr)
library(car)
library(corrplot)
library(ggfortify)
library(GGally)
library(Hmisc)
library(psych)
library(kableExtra)
library(modelr)  # for computing rmse, mae
library(caret) # suitable for generating rmse, R2 and mae. Type ?caret::RMSE.. for more info
```










<!-- Introduction  -->

Introduction
=============

As the world emerges into a global village spearheaded by the recent technological advancements, people from all walks of life are judiciously investing in education. It comes with no surprise that the United States (US) is ranked first in education since 2020. The quality of education in the US coupled with her numerous opportunities attracts a lot students annually. According to [open doors](https://www.iie.org/), China and India are two of the world's populous countries that send a large number of students to the US for studies.

To be a successful candidate in the admission recruitment, several factors are considered. First, standardized test scores such as the Graduate Record Examination (GRE) and Test of English as a Foreign Language (TOEFL) are required by most graduate programs. Second, letters of recommendation and statement of purpose provides admission committee information about the applicant's strengths and weaknesses, achievements and their ability to thrive in graduate school. Good undergraduate grade point average (GPA) and research experience are also essential. An applicant with a good GRE score, TOEFL score, high undergraduate GPA, has research experience with a strong recommendation letters and statement of purpose is likely to get admission easily. 

In this study, I examined some of the factors mentioned above and their effect on an Indian applicant's ability to gain admission in a US university's graduate school. 


## Research question and statement

The current study, however, is guided by the following research question and statement.

1. Design a model for predicting an Indian applicant's chances of getting admission into a graduate school in the US.

2.  Does 
      a. GRE scores,
      b. TOEFL scores,
      c. University ranking,
      d. Letters of recommendation,
      e. Statement of purpose,
      f. Undergraduate GPA, and
      g. Research experience,
    influence an Indian applicant's chances of getting admission into a graduate school in the US?
 







<!-- Data  -->

Data
====

This data was created to predict graduate admission of Indian students into an American university. The data contain several influential variables necessary to trigger admission for masterâ€™s programs. Table \ref{tab:tab1} summarizes the data characteristics. It has seven continuous and one discrete variable. Admission is measured as a probability with higher values indicating the posibility of getting an admission. Other variables such as GRE score or research experience (1 = applicant has research experience and 0 otherwise) and whether university ranking is important (1 = high ranking to 5 = low ranking). Because the data has been preclean, I proceeded with the analysis. I pulled this data set from Kaggle (www.kaggle.com), a data science competition website.

```{r results='markup', echo=FALSE}
dt <- read.csv('dt.csv') 
kbl(dt, caption = "Characteristics of the data\\label{tab:tab1}", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```








<!-- Data Exploration  -->

Data Exploration
================

Data exploration provides important information about the data. But first, I loaded the data into R and preview the responses of the first six applicants (see Table \ref{tab:tab2}). For example, the first applicant chose a low ranking university despite having high GRE and TOEFL score of 337 and 118 respectively, an undergraduate GPA of 9.65 (out of 10), has research experience and statement of purpose and letters of recommendation rated 4.5 (out of 5) each. From the result, this applicant has 92% chance of getting admission for a master's program in the US.

```{r, echo=TRUE}
data <- read.csv('data.csv') 
kbl(head(data), caption = "Sample data preview\\label{tab:tab2}", booktabs = T) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
```

Because the purpose of this study is to build a regression model for prediction and examine the factors that influence admission chances, I splitted the data into two: `train` and `test` data. The `train` data will be used to build and train the regression model, while the `test` data will validate the model's prediction accuracy. So, $85\%$ of the data will be used for the training. This represents 425 applicants. The remaining 75 applicants' responses will be used for prediction on the `test` data.

```{r, echo=TRUE}
# Sample size determination
nrow(data)

# Splitting data into test and train
# 85% of the sample size for training
sample_size <- floor(.85 * nrow(data))
sample_size

set.seed(558)  # make data reproducible
# generate random sample without replacement 
train_ind <- sample(seq_len(nrow(data)), size = sample_size)
# generate data of responses according to the sample
train <- data[train_ind, ]
# generates the remaining 20% of the sample
test <- data[-train_ind, ] 
```

Table \ref{tab:tab3} represents a preview of the training data set.

```{r, echo=TRUE}
kbl(head(train), caption = "Sample of the training data\\label{tab:tab3}", booktabs = T) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
```

Table \ref{tab:tab4} represents a preview of the testing data set.

```{r, echo=TRUE}
kbl(head(test), caption = "Sample of the test data\\label{tab:tab4}", booktabs = T) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
```

For the remaining part of the study, the training data will be used for further exploration and analysis.



## Data description

The `describe` function from the `psych` package provides a summary information such as the sample size (n), mean, standard deviation (sd) among others. For example, in Table \ref{tab:tab5} the best applicant(s) has(have) $97\%$ of getting admission and the average admission rate is $72\%$. Also, the average test scores obtained by these applicants is approximately 317 and 107 for GRE and TOEFL respectively. Similarly, the statement of purpose and letters of recommendation are above average with an undergraduate GPA exceeding 8.0 (on a scale of 10.0). Thus, to a large extent, the applicants possesses good qualities for admission. 

```{r, echo=TRUE}
des_stats <- psych::describe(train)
kbl(des_stats, caption = "Descriptive statistics of applicants\\label{tab:tab5}",
    booktabs = T) %>%
    kable_styling(latex_options = c("striped", "scale_down"))
```

Table \ref{tab:tab6} provides further information on the number and percentage of applicants who prioritize university ranking. About $34\%$ of the applicants prefer averagely ranked university. Meanwhile, about twice the number of applicants who chose highly ranked universities prefer the least ranked ones to increase their admission intake.

```{r, echo=TRUE}
urank <- count(train, uranking)
ud <- data.frame(
  University_ranking = urank$uranking,
  Number_of_applicants = urank$n,
  Percentage_of_applicants = round(urank$n/sum(urank$n)*100,2)
)
kbl(ud, caption = "Aplicants' choice of university ranking\\label{tab:tab6}", 
    booktabs = T) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
```

In as much as university rankng is crucial, research experience can be pivotal and may give an applicant a competitive edge. It can be seen that more than half of the applicants have research experience (see Table \ref{tab:tab7}). 

```{r, echo=TRUE}
res_exp <- count(train, research)
rexp <- data.frame(
  Research_experience = res_exp$research,
  Number_of_applicants = res_exp$n,
  Percentage_of_applicants = round(res_exp$n/sum(res_exp$n)*100,2)
)
kbl(rexp, 
    caption = "Distribution of Aplicants by research experience\\label{tab:tab7}",
    booktabs = T) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
```

Again, I explored the relationship between research experience and university ranking. The crosstabulation provides insightful information. 

```{r, echo=TRUE}
suv <- data.frame(train$research, train$uranking)
kbl(table(suv), 
  caption = "Crosstabulation of unviersity ranking and research experience\\label{tab:tab8}",
  booktabs = T) %>%
  kable_styling() %>%
  pack_rows("Research Experience", 1,2) %>%
  add_indent(c(1,2), level_of_indent = 12) %>%
  add_header_above(c(" " = 1, "University Ranking" = 5))
```

It is surprising that there is an excess of 45 applicants with research experience who applied to a low ranked university. In contrast, 42 more applicants with no research experience chose the second most rated universities. Several reasons may account for this decisions ranging from program of choice, application documents to funding.

```{r figs, echo=TRUE, fig.cap="\\label{fig:newfig1} University ranking by level of research experience"}
# Graphical representation
ggplot(train, aes(uranking)) + geom_histogram(bins = 10, binwidth = 0.5) + 
  xlab("University ranking") + ylab("Number of students") +
  facet_wrap(~research)
```

## Plots and Correlation analysis

Correlation analysis shows the strength of the relationship between variables. It ranges from $-1$ to $1$ and correlation coefficients closer to $-1$ or $1$ are signs of strong correlation. For this analysis, I am interested in exploring the correlation between admission chance and all the other variables. From Table \ref{tab:tab27}, there exist a significant, positive and moderate to strong correlation coefficients ranging from $0.548$ to $0.890$. The highest of this is the undergraduate GPA, followed by GRE score and TOEFL score. 




```{r echo=FALSE, message=FALSE, warning=FALSE}
## CODE ADAPTED FROM http://www.sthda.com/english/wiki/elegant-correlation-table-using-xtable-r-package
# x is a matrix containing the data
# method : correlation method. "pearson"" or "spearman"" is supported
# removeTriangle : remove upper or lower triangle
# results :  if "html" or "latex"
  # the results will be displayed in html or latex format
rcorrst <-function(x, method=c("pearson", "spearman"), removeTriangle=c("upper", "lower"),
                     result=c("none", "html", "latex")){
    #Compute correlation matrix
    require(Hmisc)
    x <- as.matrix(x)
    correlation_matrix<-rcorr(x, type=method[1])
    R <- correlation_matrix$r # Matrix of correlation coeficients
    p <- correlation_matrix$P # Matrix of p-value 
    
    ## Define notions for significance levels; spacing is important.
    mystars <- ifelse(p < .0001, "****", ifelse(p < .001, "*** ", ifelse(p < .01, "**  ", ifelse(p < .05, "*   ", "    "))))
    
    ## trunctuate the correlation matrix to two decimal
    R <- format(round(cbind(rep(-1.11, ncol(x)), R), 2))[,-1]
    
    ## build a new matrix that includes the correlations with their apropriate stars
    Rnew <- matrix(paste(R, mystars, sep=""), ncol=ncol(x))
    diag(Rnew) <- paste(diag(R), " ", sep="")
    rownames(Rnew) <- colnames(x)
    colnames(Rnew) <- paste(colnames(x), "", sep="")
    
    ## remove upper triangle of correlation matrix
    if(removeTriangle[1]=="upper"){
      Rnew <- as.matrix(Rnew)
      Rnew[upper.tri(Rnew, diag = TRUE)] <- ""
      Rnew <- as.data.frame(Rnew)
    }
    
    ## remove lower triangle of correlation matrix
    else if(removeTriangle[1]=="lower"){
      Rnew <- as.matrix(Rnew)
      Rnew[lower.tri(Rnew, diag = TRUE)] <- ""
      Rnew <- as.data.frame(Rnew)
    }
    
    ## remove last column and return the correlation matrix
    Rnew <- cbind(Rnew[1:length(Rnew)-1])
    if (result[1]=="none") return(Rnew)
    else{
      if(result[1]=="html") print(xtable(Rnew), type="html")
      else print(xtable(Rnew), type="latex") 
    }
} 
```



```{r echo=TRUE, message=FALSE, warning=FALSE}
toyota <- rcorrst(train[, 2:9])
# correlation with p values
kbl(toyota, 
    caption = "Correlation with p values\\label{tab:tab27}",
    booktabs = T) %>% 
    kable_styling(latex_options = c("striped", "hold_position")) %>% 
    footnote(general = "**** p < .0001",
    footnote_as_chunk = T
)
```









<!-- Building a regression model  -->


Building a regression model
===========================

In this section, I built a multiple regression model for predicting admission chance. Admission chance is the dependent variable while the other variables represent the independent variables. Equation  represents the model. 

$$
y = \beta_{0} + \sum_{i=1}^{7} \beta_{i}x_{i} + \epsilon
$$

where $y$ is the admission chance, $\beta_{0}$ is the intercept,$\beta_{i}$ are the independent variables and $\epsilon$ represents the error term.

The R code below execute the model. Before viewing the results, I examined the assumptions of linear regression.

```{r echo=TRUE, warning=FALSE}
reg_model <- lm(admit_chance ~ gre + toefl + uranking + sop + lor + cgpa + research, 
                data = train)
```

## Regression diagnostics

It is very important to examine the regression diagnostic and address possible problems before making decisions. The following assumptions are examined. I used the `autoplot` function in the `ggfortify` package to generate the plots. For multicollinearity, I extracted the variance inflation factor function `(vif)` from the `car` package.

### Linearity 

From the residual vs fitted plot below, there is no distinct pattern. In other words, the data points are randomly and evenly dispersed about the reference line, this means the relationship is linear. 

### Normal Q-Q plot

The plot shows that the normality assumption has been met as the data points do not deviate extremely from the normal probability line. Also, the values of the skewness and kurtosis for the variables fall within the interval $\pm 2$, indicating the non-violation of normality assumption (see Table \ref{tab:tab5}).

### Homoscedasticity 

The scale-location plot indicate spread of the data and it is used to check the homogeneity of variance of the residuals. The residuals are well spread but decreases slightly along the fitted values. Thus, homoscedasticity is satisfied.

### Outliers

The residuals vs leverage provide a good information on the influential variables in the regression result. The absence of cook distance line on the plot is situation where outliers are not present in the study.

```{r echo=TRUE, warning=FALSE}
autoplot(reg_model)
```

### Multicolinearity 

This can be done by using the vif in the `car package`. I computed the variance inflation factor (VIF) and tolerance (1/VIF) to check if there is a high correlation between the independent variables. A rule of thumb for interpreting the variance inflation factor is:

+   1 = not correlated
+   Between 1 and 5 = moderately correlated and
+   Greater than 5 = highly correlated

The results show a moderate correlation between the independent variables (see Table \ref{tab:tab10}).

```{r echo=TRUE, warning=FALSE}
vit <- data.frame(
  VIF = round(car::vif(reg_model),2),
  Tolerance = round(1/car::vif(reg_model),2)
)
kbl(vit, caption = "VIF and Tolerance values\\label{tab:tab10}",
    booktabs = T) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
```

From the above discourse, it is evident that the regression assumptions have been satisfied.

Now, the regression model is represented below.

```{r echo=TRUE, warning=FALSE}
summary(reg_model)
```

Though the regression output produces substantive information about the R-squared and adjusted R-squared, I also examined the Root Mean Squared Error (RMSE) and the Mean Absolute Error (MAE) to ensure the data adequately fit the model.

```{r echo=TRUE, warning=FALSE}
# Model performance
mod <- data.frame(
  RMSE = round(rmse(reg_model, data = train),4),
  MAE = round(mae(reg_model, data = train),4)
)
kbl(mod, caption = "Model performance\\label{tab:tab11}",
    booktabs = T) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
```

## Prediction

As stated in the introduction, the purpose of this study is to predict admission chance of a prospective applicant into a university in the US. At this point, I am going to use the `test` data for the prediction. The first six responses of the data is shown in Table \ref{tab:tab13}. 

```{r echo=TRUE, warning=FALSE}
kbl(head(test), caption = "Sample of test data\\label{tab:tab13}",
    booktabs = T) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
```

Table \ref{tab:tab15} and \ref{tab:tab16}  provide information on the performance metrics for the predictive model.

```{r echo=TRUE, warning=FALSE}
prediction <- data.frame(predicted_admit_chance = predict(reg_model, test), 
          admit_chance  = test$admit_chance
           )
pred <- data.frame(
  RMSE = RMSE(prediction$predicted_admit_chance, prediction$admit_chance),
  MAE = MAE(prediction$predicted_admit_chance, prediction$admit_chance),
  R_Square = R2(prediction$predicted_admit_chance, prediction$admit_chance)
)
kbl(round(pred,4), caption = "Model performance of predicted model\\label{tab:tab15}",
    booktabs = T) %>%
    kable_styling(latex_options = c("striped", "hold_position"))

# predictive accuracy
Correlat <-  rcorrst(prediction) 
kbl(Correlat, caption = "Predictive Accuracy\\label{tab:tab16}",
    booktabs = T) %>%
    kable_styling(latex_options = c("striped", "hold_position")) %>%
    footnote(general = "**** p < .0001",
    footnote_as_chunk = T)
```

Now, the predicted and actual rate of admission chance is shown in Table \ref{tab:tab17}.

```{r echo=TRUE, warning=FALSE}
kbl(head(prediction), caption = "Model performance of predicted model\\label{tab:tab17}",
    booktabs = T) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
```







<!-- Discussion and limitations  -->


Discussions and limitations
===========================

The findings obtained from the analysis are intriguing. First, the model account for $81.5\%$ of the variability in applicants' chance of getting admission into a US university for a graduate program. From the outcome of RMSE and MAE, the model resulted in respectively 5.8% and 4.2% error values. These are good values which validates the performance of the study model. Second, I observed that, among the independent variables, university ranking $(\beta=.0060,\, p > 0.05)$ and statement of purpose $(\beta=.0061,\, p > 0.05)$ have no significant effect on an applicant's admission chance. In other words, the university ranking and SOP does not influence an individual's chances of getting admission.  However, GRE scores $(\beta=.0016,\, p < 0.01)$, TOEFL score $(\beta=.0028,\, p < 0.01)$, letters of recommendation $(\beta=.0141,\, p < 0.01)$, undergraduate GPA $(\beta=.120,\, p < 0.001)$ and research experience $(\beta=.0274,\, p < 0.001)$ have positive and significant impact on an applicant's admission chance. This means applicants from India must prioritize these influential variables when considering graduate studies in the United States.

The model is also designed to predict the admission chances of applicants when it is fed with required information. For a group of 75 applicants, the model has an accuracy rate of 87.9% and accounted for 77.2% of the variability in the admission chance with low error rate.

Inspite of the relevant findings obtained, the study is not without limitations. First, insufficient information was provided on the scale used for rating statement of purpose, letters of recommendation, admission chance etc. Data was not collected on whether admissions into these university were funded or not. Though challenging, it is quite easier for an applicant with good grades to get admission into a US graduate school  without funding. So, the findings of this study must be interpreted with caution. 







<!-- Conclusion  -->

Conclusion
============

I conclude by entreating Indians who desire to study in the US to have a very high CGPA in their undergraduate program, good research experience and they should never forget to rely on Professors who can provide efficient and convincing recommendation letters. A good GRE and/or TOEFL score will enhance their chances of gaining admission into a graduate school in the US.






<!-- Reference  -->

\newpage


Reference {-}
==============

---
nocite: "@*"
---
